【K-近邻算法】
k-近邻（kNN,k-NearestNeighbor）算法是一种基本分类与回归方法，我们这里只讨论分类问题中的 k-近邻算法。k-近邻算法的输入为实例的特征向量，对应于特征空间的点；输出为实例的类别，可以取多类。k-邻算法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其 k 个最近邻的训练实例的类别，通过多数表决等方式进行预测。因此，k近邻算法不具有显式的学习过程。k近邻算法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。k值的选择、距离度量以及分类决策规则是k近邻算法的三个基本要素。

优点：精度高、对异常值不敏感、无数据输入假定
缺点：计算复杂度高、空间复杂度好
适用数据范围：数值型和标称型

KNN 工作原理
1 假设有一个带有标签的样本数据集（训练样本集），其中包含每条数据与所属分类的对应关系。
2 输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较。
3 计算新数据与样本数据集中每条数据的距离。
4 对求得的所有距离进行排序（从小到大，越小表示越相似）。
5 取前 k （k 一般小于等于 20 ）个样本数据对应的分类标签。
6 求 k 个数据中出现次数最多的分类标签作为新数据的分类。

算法流程：
1 搜集数据
2 准备数据：格式化处理
3 分析数据
4 训练数据：不适用于KNN
5 测试算法：评价指标，如计算错误率
6 使用算法：

算法思想：
1 计算已知类别中数据集的点与当前点的距离
2 按照距离递增次序排序
3 选取与当前点距离最小的k个点
4 确定前k个点所在类别的出现频率
5 返回前k个点出现频率最高的类别作为当前点的预测分类


【优化约会网站的配对效果】

海伦使用约会网站寻找约会对象。经过一段时间之后，她发现曾交往过三种类型的人:
1 不喜欢的人
2 魅力一般的人
3 极具魅力的人

她希望：工作日与魅力一般的人约会、周末与极具魅力的人约会、不喜欢的人则直接排除掉。现在她收集到了一些约会网站未曾记录的数据信息，这更有助于匹配对象的归类。
样本特征：
1 每年获得飞行常客里程数
2 玩视频游戏所耗时间百分比
3 每周消费的冰淇淋公升数

开发流程
收集数据：提供文本文件
准备数据：使用 Python 解析文本文件
分析数据：使用 Matplotlib 画二维散点图
训练算法：此步骤不适用于 k-近邻算法
测试算法：使用海伦提供的部分数据作为测试样本。
        测试样本和非测试样本的区别在于：
            测试样本是已经完成分类的数据，如果预测分类与实际类别不同，则标记为一个错误。
使用算法：产生简单的命令行程序，然后海伦可以输入一些特征数据以判断对方是否为自己喜欢的类型。



【TF-IDF】 bag-of-word 方法
1 分词，词频统计
2 过滤停用词
3 逆文档频率，常见词小权重，不常见较大权重
4 TF_IDF：词频*逆文档频率
5 排序得到关键词

[提取关键字]
1 计算词频
    词频（TF）= 某个词在文章中出现的次数
    考虑文章长短之分，便于不同文章比较，进行词频标准化
    词频（TF）= 某个词在文章中出现的次数/文章的总词数
    或者
    词频（TF）= 某个词在文章中出现的次数/该文出现词数最多的词的出现词数

2 计算逆文档频率
    IDF = log（语料库的文档总数/包含该词的文档数+1）
    
3 计算TF-IDF
    TF-IDF = TF * IDF

【相似度计算】
1 分词
2 列出所有词
3 计算词频
4 写出词频向量
5 余弦定理计算相似度


【相似度计算】
　　（1）使用TF-IDF算法，找出两篇文章的关键词；
　　（2）每篇文章各取出若干个关键词（比如20个），合并成一个集合，计算每篇文章对于这个集合中的词的词频（为了避免文章长度的差异，可以使用相对词频）；
　　（3）生成两篇文章各自的词频向量；
　　（4）计算两个向量的余弦相似度，值越大就表示越相似。

